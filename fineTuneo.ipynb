{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\ezequ\\OneDrive\\Documentos\\Facultad\\NLP\\fine tuneo\\PreProces\\clean.csv'\n",
    "df = pd.read_csv(file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AN</th>\n",
       "      <th>ECO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. d4 d5 2. c4 c6 3. e3 a6 4. Nf3 e5 5. cxd5 e...</td>\n",
       "      <td>D10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1. e4 e5 2. b3 Nf6 3. Bb2 Nc6 4. Nf3 d6 5. d3 ...</td>\n",
       "      <td>C20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1. e4 d5 2. exd5 Qxd5 3. Nf3 Bg4 4. Be2 Nf6 5....</td>\n",
       "      <td>B01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1. e3 Nf6 2. Bc4 d6 3. e4 e6 4. Nf3 Nxe4 5. Nd...</td>\n",
       "      <td>A00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. N...</td>\n",
       "      <td>B90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6254708</th>\n",
       "      <td>1. e4 c5 2. Nf3 d6 3. d4 Qa5+ 4. Bd2 Qc7 5. Bc...</td>\n",
       "      <td>B54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6254709</th>\n",
       "      <td>1. e4 e5 2. Nf3 h6 3. Nc3 d6 4. Bc4 Qf6 5. Nd5...</td>\n",
       "      <td>C40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6254710</th>\n",
       "      <td>1. e4 { [%eval 0.22] } 1... c5 { [%eval 0.35] ...</td>\n",
       "      <td>B21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6254711</th>\n",
       "      <td>1. Nf3 d5 2. c4 dxc4 3. Qa4+ c6 4. Qxc4 Nf6 5....</td>\n",
       "      <td>A09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6254712</th>\n",
       "      <td>1. e4 e6 2. d4 d6 3. Nc3 c6 4. Nf3 Be7 5. Bc4 ...</td>\n",
       "      <td>C00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6254713 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        AN  ECO\n",
       "0        1. d4 d5 2. c4 c6 3. e3 a6 4. Nf3 e5 5. cxd5 e...  D10\n",
       "1        1. e4 e5 2. b3 Nf6 3. Bb2 Nc6 4. Nf3 d6 5. d3 ...  C20\n",
       "2        1. e4 d5 2. exd5 Qxd5 3. Nf3 Bg4 4. Be2 Nf6 5....  B01\n",
       "3        1. e3 Nf6 2. Bc4 d6 3. e4 e6 4. Nf3 Nxe4 5. Nd...  A00\n",
       "4        1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. N...  B90\n",
       "...                                                    ...  ...\n",
       "6254708  1. e4 c5 2. Nf3 d6 3. d4 Qa5+ 4. Bd2 Qc7 5. Bc...  B54\n",
       "6254709  1. e4 e5 2. Nf3 h6 3. Nc3 d6 4. Bc4 Qf6 5. Nd5...  C40\n",
       "6254710  1. e4 { [%eval 0.22] } 1... c5 { [%eval 0.35] ...  B21\n",
       "6254711  1. Nf3 d5 2. c4 dxc4 3. Qa4+ c6 4. Qxc4 Nf6 5....  A09\n",
       "6254712  1. e4 e6 2. d4 d6 3. Nc3 c6 4. Nf3 Be7 5. Bc4 ...  C00\n",
       "\n",
       "[6254713 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ezequ\\OneDrive\\Documentos\\Facultad\\NLP\\fine tuneo\\fine_tuneo\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "api_token = \"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\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B', token=api_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the prompt format and EOS token\n",
    "chess_prompt = \"\"\"Analyze the following chess game played between [Player 1] (White) and [Player 2] (Black).\n",
    "\n",
    "### Game Notation (PGN):\n",
    "{}\n",
    "\n",
    "**After analyzing the game, explain the key strategic decisions made by both players throughout the game. Discuss the turning points and missed opportunities.\n",
    "\n",
    "**Additionally, identify and explain the following for each move:**\n",
    "\n",
    "* **Opening Principles:** How well did the opening choices follow sound opening principles?\n",
    "* **Tactical Opportunities:** Were there any tactical opportunities missed during the game? \n",
    "* **Positional Advantages:** How did the players maneuver their pieces to gain positional advantages?\n",
    "* **Endgame Technique:** How effectively did the players convert their advantages in the endgame?\n",
    "\n",
    "**Finally, based on your analysis, what are the key takeaways for improving one's chess game?**\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(row):\n",
    "    return chess_prompt.format(row['AN'], row['ECO']) + EOS_TOKEN\n",
    "\n",
    "# Apply formatting to the DataFrame\n",
    "df['text'] = df.apply(formatting_prompts_func, axis=1)\n",
    "\n",
    "# Convert DataFrame to dataset format expected by Hugging Face\n",
    "dataset = Dataset.from_pandas(df[['text']])\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Load the model\n",
    "model = LlamaForCausalLM.from_pretrained('llama/llama-3')\n",
    "model.to(device)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    eval_steps=5000,\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('path/to/save/finetuned-model')\n",
    "tokenizer.save_pretrained('path/to/save/finetuned-model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine_tuneo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
