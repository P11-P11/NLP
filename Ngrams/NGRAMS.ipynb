{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr8ej1Wa3HmB"
      },
      "source": [
        "N-grams Parte 1\n",
        "\n",
        "Vamos a entrenar un modelo ngrams \"from scratch\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbsta7lIHhz"
      },
      "source": [
        "## Download WikiText-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.265129Z",
          "start_time": "2020-01-29T18:30:17.460952Z"
        },
        "id": "jfGVtATnIHh2",
        "outputId": "798f4e8f-5ab0-459f-807e-782b860e354b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-16 13:54:14--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10797148 (10M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]  10.30M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-04-16 13:54:15 (139 MB/s) - ‘train.txt’ saved [10797148/10797148]\n",
            "\n",
            "--2024-04-16 13:54:15--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1121681 (1.1M) [text/plain]\n",
            "Saving to: ‘valid.txt’\n",
            "\n",
            "valid.txt           100%[===================>]   1.07M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-04-16 13:54:15 (28.8 MB/s) - ‘valid.txt’ saved [1121681/1121681]\n",
            "\n",
            "--2024-04-16 13:54:15--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1256449 (1.2M) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]   1.20M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-04-16 13:54:15 (28.5 MB/s) - ‘test.txt’ saved [1256449/1256449]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import operator\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZF0lClTV4ia",
        "outputId": "20f6c31b-7eb8-4571-e9f5-2065571ba77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparación de datos\n"
      ],
      "metadata": {
        "id": "xDB3XZLL1P0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.271133Z",
          "start_time": "2020-01-29T18:30:18.266713Z"
        },
        "id": "u5edQma8IHh7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def flatten(lst):\n",
        "    return [item for sublist in lst for item in sublist]\n",
        "\n",
        "def prepare_data(filename):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Read lines, strip them, lower them, split by spaces, and append </s> token\n",
        "    data = [l.strip().lower().split() + ['</s>'] for l in open(filename, 'r', encoding='utf-8') if l.strip()]\n",
        "\n",
        "    # Lemmatize words in data\n",
        "    data = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in data]\n",
        "\n",
        "    corpus = flatten(data)\n",
        "    vocab = set(corpus)\n",
        "    return vocab, data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvjT2fTlIHh9"
      },
      "source": [
        "La probabilidad de una palabra depende la historia inmediata de la secuencia.\n",
        "\n",
        "$P(w_t|w_{t-1}, w_{t-2}, ... , w_1)$\n",
        "\n",
        "### Markov Assumption\n",
        "\n",
        "- Unigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t)$\n",
        "- Bigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}) $\n",
        "- Trigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}, w_{t-2})$\n",
        "\n",
        "Contamos palabras para estimar las probabilidades\n",
        "\n",
        "$P(w_t|w_{t-1}, w_{t-2}, ... ,w_{t-N+1}) = \\frac{C(w_t, w_{t-1}, w_{t-2}, ... ,w_{t-N+1} )}{C(w_{t-1}, w_{t-2}, ... ,w_{t-N+1})}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VwN9CXyIHiC"
      },
      "source": [
        "## Train with toy dataset\n",
        "\n",
        "At this step, let's train a Bigram language model on the toy dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.290242Z",
          "start_time": "2020-01-29T18:30:18.272506Z"
        },
        "id": "Ql01Z8IiIHh_"
      },
      "outputs": [],
      "source": [
        "class NGramLM():\n",
        "    def __init__(self, N):\n",
        "        self.N = N\n",
        "        self.vocab = set()\n",
        "        self.data = []\n",
        "        self.prob = {}\n",
        "        self.counts = defaultdict(Counter)\n",
        "\n",
        "    def train(self, vocab, data, smoothing_k=0):\n",
        "        self.vocab = vocab\n",
        "        self.data = data\n",
        "        self.smoothing_k = smoothing_k\n",
        "\n",
        "        if self.N == 1:\n",
        "            self.counts = Counter(flatten(data))\n",
        "            self.prob = self.get_prob(self.counts)\n",
        "        else:\n",
        "            self.vocab.add('<s>')\n",
        "            counts = self.count_ngram()\n",
        "\n",
        "            self.prob = {}\n",
        "            for context, counter in counts.items():\n",
        "                self.prob[context] = self.get_prob(counter)\n",
        "\n",
        "    def count_ngram(self):\n",
        "        counts = defaultdict(Counter)\n",
        "        for sentence in self.data:\n",
        "            sentence = (self.N - 1) * ['<s>'] + sentence\n",
        "            for i in range(len(sentence)-self.N+1):\n",
        "                context = sentence[i:i+self.N-1]\n",
        "                context = \" \".join(context)\n",
        "                word = sentence[i+self.N-1]\n",
        "                counts[context][word] += 1\n",
        "\n",
        "        self.counts = counts\n",
        "        return counts\n",
        "\n",
        "    def get_prob(self, counter):\n",
        "        total = float(sum(counter.values()))\n",
        "        k = self.smoothing_k\n",
        "\n",
        "        prob = {}\n",
        "        for word, count in counter.items():\n",
        "            prob[word] = (count + k) / (total + len(self.vocab) * k)\n",
        "        return prob\n",
        "\n",
        "    def get_ngram_logprob(self, word, seq_len, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return math.log(self.prob[word]) / seq_len\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return math.log(self.prob[context][word]) / seq_len\n",
        "        else:\n",
        "            # assign a small probability to the unseen ngram\n",
        "            # to avoid log of zero and to penalise unseen word or context\n",
        "            return math.log(1/len(self.vocab)) / seq_len\n",
        "\n",
        "    def get_ngram_prob(self, word, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return self.prob[word]\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return self.prob[context][word]\n",
        "        elif word in self.vocab and self.smoothing_k > 0:\n",
        "            # probability assigned by smoothing\n",
        "            return self.smoothing_k / (sum(self.counts[context].values()) + self.smoothing_k*len(self.vocab))\n",
        "        else:\n",
        "            # unseen word or context\n",
        "            return 0\n",
        "\n",
        "    # In this method, the perplexity is measured at the sentence-level, averaging over all sentences.\n",
        "    # Actually, it is also possible to calculate perplexity by merging all sentences into a long one.\n",
        "    def perplexity(self, test_data):\n",
        "        log_ppl = 0\n",
        "        if self.N == 1:\n",
        "            for sentence in test_data:\n",
        "                for word in sentence:\n",
        "                    log_ppl += self.get_ngram_logprob(word=word, seq_len=len(sentence))\n",
        "        else:\n",
        "            for sentence in test_data:\n",
        "                for i in range(len(sentence)-self.N+1):\n",
        "                    context = sentence[i:i+self.N-1]\n",
        "                    context = \" \".join(context)\n",
        "                    word = sentence[i+self.N-1]\n",
        "                    log_ppl += self.get_ngram_logprob(context=context, word=word, seq_len=len(sentence))\n",
        "\n",
        "        log_ppl /= len(test_data)\n",
        "        ppl = math.exp(-log_ppl)\n",
        "        return ppl\n",
        "\n",
        "    def _is_unseen_ngram(self, context, word):\n",
        "        if context not in self.prob.keys() or word not in self.prob[context].keys():\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    # generate the most probable k words\n",
        "    def generate_next(self, context, k):\n",
        "        context = (self.N-1) * '<s> ' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "\n",
        "        if ngram_context in self.prob.keys():\n",
        "            candidates = self.prob[ngram_context]\n",
        "            most_probable_words = sorted(candidates.items(), key=lambda kv: kv[1], reverse=True)\n",
        "            for i in range(min(k, len(most_probable_words))):\n",
        "                print(\" \".join(context[self.N-1:])+\" \"+most_probable_words[i][0]+\"\\t P={}\".format(most_probable_words[i][1]))\n",
        "        else:\n",
        "            print(\"Unseen context!\")\n",
        "\n",
        "    # generate the next n words with greedy search Poner en un slide\n",
        "    def generate_next_n(self, context, n):\n",
        "        context = (self.N-1) * '<s> ' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "\n",
        "        for i in range(n):\n",
        "            try:\n",
        "                candidates = self.prob[ngram_context]\n",
        "                most_likely_next = max(candidates.items(), key=operator.itemgetter(1))[0]\n",
        "                context += [most_likely_next]\n",
        "                ngram_context_list = ngram_context_list[1:] + [most_likely_next]\n",
        "                ngram_context = \" \".join(ngram_context_list)\n",
        "            except:\n",
        "                break\n",
        "        print(\" \".join(context[self.N-1:]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.297355Z",
          "start_time": "2020-01-29T18:30:18.291387Z"
        },
        "id": "sabDGGntIHiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58687f3f-c2e3-4d6a-cce5-6800544229f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'like', 'ice', 'cream', '</s>'], ['i', 'like', 'chocolate', '</s>'], ['i', 'hate', 'beans', '</s>']]\n",
            "{'ice', 'like', 'chocolate', '</s>', 'i', 'cream', 'beans', 'hate'}\n"
          ]
        }
      ],
      "source": [
        "corpus = [\"I like ice cream\",\n",
        "         \"I like chocolate\",\n",
        "         \"I hate beans\"]\n",
        "data = [l.strip().lower().split() + ['</s>'] for l in corpus if l.strip()]\n",
        "vocab = set(flatten(data))\n",
        "print(data)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.301895Z",
          "start_time": "2020-01-29T18:30:18.298317Z"
        },
        "id": "-HKqP6zvIHiE"
      },
      "outputs": [],
      "source": [
        "def print_probability(lm):\n",
        "    for context in lm.vocab:\n",
        "        for word in lm.vocab:\n",
        "            prob = lm.get_ngram_prob(word, context)\n",
        "            print(\"P({}\\t|{}) = {}\".format(word, context, prob))\n",
        "        print(\"--------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y-jHF6bIHiF"
      },
      "source": [
        "## Smoothing\n",
        "Smoothing se utiliza para contrarrestar la dispersión en el Modelo de Lenguaje N-Gram. La masa de probabilidad se desplaza hacia las palabras menos frecuentes.\n",
        "\n",
        "\n",
        "$$P(w_t | context) = \\frac{C(w_t, context)+1}{C(context)+|V|}$$\n",
        "\n",
        "El problema es que si hay muchas palabras con 0, entonces las palabras mas frecuentas sacrificarán más probabilidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.310269Z",
          "start_time": "2020-01-29T18:30:18.302761Z"
        },
        "id": "nBS-MIWiIHiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d26db3-adda-4234-d44e-36a359cdf620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(ice\t|ice) = 0\n",
            "P(like\t|ice) = 0\n",
            "P(chocolate\t|ice) = 0\n",
            "P(<s>\t|ice) = 0\n",
            "P(</s>\t|ice) = 0\n",
            "P(i\t|ice) = 0\n",
            "P(cream\t|ice) = 1.0\n",
            "P(beans\t|ice) = 0\n",
            "P(hate\t|ice) = 0\n",
            "--------------------------\n",
            "P(ice\t|like) = 0.5\n",
            "P(like\t|like) = 0\n",
            "P(chocolate\t|like) = 0.5\n",
            "P(<s>\t|like) = 0\n",
            "P(</s>\t|like) = 0\n",
            "P(i\t|like) = 0\n",
            "P(cream\t|like) = 0\n",
            "P(beans\t|like) = 0\n",
            "P(hate\t|like) = 0\n",
            "--------------------------\n",
            "P(ice\t|chocolate) = 0\n",
            "P(like\t|chocolate) = 0\n",
            "P(chocolate\t|chocolate) = 0\n",
            "P(<s>\t|chocolate) = 0\n",
            "P(</s>\t|chocolate) = 1.0\n",
            "P(i\t|chocolate) = 0\n",
            "P(cream\t|chocolate) = 0\n",
            "P(beans\t|chocolate) = 0\n",
            "P(hate\t|chocolate) = 0\n",
            "--------------------------\n",
            "P(ice\t|<s>) = 0\n",
            "P(like\t|<s>) = 0\n",
            "P(chocolate\t|<s>) = 0\n",
            "P(<s>\t|<s>) = 0\n",
            "P(</s>\t|<s>) = 0\n",
            "P(i\t|<s>) = 1.0\n",
            "P(cream\t|<s>) = 0\n",
            "P(beans\t|<s>) = 0\n",
            "P(hate\t|<s>) = 0\n",
            "--------------------------\n",
            "P(ice\t|</s>) = 0\n",
            "P(like\t|</s>) = 0\n",
            "P(chocolate\t|</s>) = 0\n",
            "P(<s>\t|</s>) = 0\n",
            "P(</s>\t|</s>) = 0\n",
            "P(i\t|</s>) = 0\n",
            "P(cream\t|</s>) = 0\n",
            "P(beans\t|</s>) = 0\n",
            "P(hate\t|</s>) = 0\n",
            "--------------------------\n",
            "P(ice\t|i) = 0\n",
            "P(like\t|i) = 0.6666666666666666\n",
            "P(chocolate\t|i) = 0\n",
            "P(<s>\t|i) = 0\n",
            "P(</s>\t|i) = 0\n",
            "P(i\t|i) = 0\n",
            "P(cream\t|i) = 0\n",
            "P(beans\t|i) = 0\n",
            "P(hate\t|i) = 0.3333333333333333\n",
            "--------------------------\n",
            "P(ice\t|cream) = 0\n",
            "P(like\t|cream) = 0\n",
            "P(chocolate\t|cream) = 0\n",
            "P(<s>\t|cream) = 0\n",
            "P(</s>\t|cream) = 1.0\n",
            "P(i\t|cream) = 0\n",
            "P(cream\t|cream) = 0\n",
            "P(beans\t|cream) = 0\n",
            "P(hate\t|cream) = 0\n",
            "--------------------------\n",
            "P(ice\t|beans) = 0\n",
            "P(like\t|beans) = 0\n",
            "P(chocolate\t|beans) = 0\n",
            "P(<s>\t|beans) = 0\n",
            "P(</s>\t|beans) = 1.0\n",
            "P(i\t|beans) = 0\n",
            "P(cream\t|beans) = 0\n",
            "P(beans\t|beans) = 0\n",
            "P(hate\t|beans) = 0\n",
            "--------------------------\n",
            "P(ice\t|hate) = 0\n",
            "P(like\t|hate) = 0\n",
            "P(chocolate\t|hate) = 0\n",
            "P(<s>\t|hate) = 0\n",
            "P(</s>\t|hate) = 0\n",
            "P(i\t|hate) = 0\n",
            "P(cream\t|hate) = 0\n",
            "P(beans\t|hate) = 1.0\n",
            "P(hate\t|hate) = 0\n",
            "--------------------------\n"
          ]
        }
      ],
      "source": [
        "lm = NGramLM(2)\n",
        "lm.train(vocab, data, smoothing_k=0)\n",
        "\n",
        "print_probability(lm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG35kkhVIHiH"
      },
      "source": [
        "## Entrenando en WikiText-2 dataset and calculando perplexity\n",
        "### Evaluating perplexity\n",
        "\n",
        "$ PPL(W) = P(w_1, w_2, ... , w_n)^{-\\frac{1}{n}}$\n",
        "\n",
        "$ log(PPL(W)) = -\\frac{1}{n}\\sum^n_{k=1}log(P(w_k|w_1, w_2, ... , w_{k-1}))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.712903Z",
          "start_time": "2020-01-29T18:30:18.312677Z"
        },
        "id": "sWJEjssXIHiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefd45af-0c0c-455c-e732-387dbdb169d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25580\n"
          ]
        }
      ],
      "source": [
        "vocab, train_data = prepare_data('train.txt')\n",
        "_, valid_data = prepare_data('valid.txt')\n",
        "_, test_data = prepare_data('test.txt')\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.224402Z",
          "start_time": "2020-01-29T18:30:18.714081Z"
        },
        "id": "rYcGOj6cIHiI"
      },
      "outputs": [],
      "source": [
        "lm = NGramLM(3)\n",
        "lm.train(vocab, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.811004Z",
          "start_time": "2020-01-29T18:30:22.225566Z"
        },
        "id": "Qh0NmO16IHiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961e7ded-c817-400d-e520-867ab492af90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1519542711642219\n",
            "1407.4468802577903\n",
            "1087.8162622740592\n"
          ]
        }
      ],
      "source": [
        "print(lm.perplexity(train_data))\n",
        "print(lm.perplexity(valid_data))\n",
        "print(lm.perplexity(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw9EMl6XIHiK"
      },
      "source": [
        "## Generando Texto\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.815241Z",
          "start_time": "2020-01-29T18:30:22.812141Z"
        },
        "id": "LVE1LyfxIHiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cae9baf-4e66-4c21-c062-31dfa1ab29df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the egg hatch at night , and the larva swim to the water surface where they drift with the ocean current , preying on <unk> . this stage involves three <unk> and last for 15 – 35 day . after the third moult , the juvenile take on a form closer to the adult , and adopts a <unk> lifestyle . the juvenile are rarely seen in the wild , and are poorly known , although they are known to be capable of digging extensive burrow . it is estimated that only 1 larva in every 20 @,@ 000 survives to the <unk> phase . when they reach a carapace length of 15 mm ( 0 @.@ 59 in ) , the juvenile leave their burrow and start their adult life . </s>\n",
            "Unseen context!\n"
          ]
        }
      ],
      "source": [
        "print(\" \".join(valid_data[12]))\n",
        "\n",
        "context = \"the eggs hatch at night , and the larvae swim to the water surface where they\"\n",
        "lm.generate_next(context, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1y3MW9DZ-nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4edd6138-3ab5-48ea-e9fa-47376d393379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unseen context!\n",
            "---\n",
            "the first\t P=0.03398926654740608\n",
            "the <unk>\t P=0.020274299344066785\n",
            "the episode\t P=0.015802027429934407\n",
            "---\n",
            " =\t P=0.26132873311734756\n",
            " the\t P=0.14112004039214035\n",
            " in\t P=0.06353347077881095\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "\n",
        "contexts = [\"the eggs\",\n",
        "            \"the\",\n",
        "            \"\"]\n",
        "for context in contexts:\n",
        "  lm.generate_next(context, 3)\n",
        "  print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.828378Z",
          "start_time": "2020-01-29T18:30:22.816839Z"
        },
        "id": "sIiRSWPoIHiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6392ff5f-c65a-4da4-bc8b-6a9682d79be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk>\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk> of the <unk> of the <unk> of the <unk> of\n"
          ]
        }
      ],
      "source": [
        "context = \"the eggs hatch at night , and the larvae swim to the water surface where they\"\n",
        "\n",
        "lm.generate_next_n(context, 10)\n",
        "\n",
        "\n",
        "lm.generate_next_n(context, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSEVmuouIHiN"
      },
      "source": [
        "Effecto de N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:41.930826Z",
          "start_time": "2020-01-29T18:30:22.829892Z"
        },
        "id": "wLVeKE_JIHiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cc927c2-67b3-410c-ace5-cb83b8f5695c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************\n",
            "1-gram LM perplexity on train set: 646.3365800668498\n",
            "1-gram LM perplexity on valid set: 546.7786237397039\n",
            "1-gram LM perplexity on test  set: 505.95615743496415\n",
            "************************\n",
            "2-gram LM perplexity on train set: 49.40224900454232\n",
            "2-gram LM perplexity on valid set: 121.9653187945607\n",
            "2-gram LM perplexity on test  set: 106.71637569105857\n",
            "************************\n",
            "3-gram LM perplexity on train set: 6.250848177738436\n",
            "3-gram LM perplexity on valid set: 340.3955446416988\n",
            "3-gram LM perplexity on test  set: 275.8197272910593\n",
            "************************\n",
            "4-gram LM perplexity on train set: 1.7915093585002564\n",
            "4-gram LM perplexity on valid set: 974.7081693443893\n",
            "4-gram LM perplexity on test  set: 758.0041880544288\n",
            "************************\n",
            "5-gram LM perplexity on train set: 1.1519542711642219\n",
            "5-gram LM perplexity on valid set: 1407.4468802577903\n",
            "5-gram LM perplexity on test  set: 1087.8162622740592\n"
          ]
        }
      ],
      "source": [
        "for n in range(1,6):\n",
        "    lm = NGramLM(n)\n",
        "    lm.train(vocab, train_data)\n",
        "    print(\"************************\")\n",
        "    print(\"{}-gram LM perplexity on train set: {}\".format(n, lm.perplexity(train_data)))\n",
        "    print(\"{}-gram LM perplexity on valid set: {}\".format(n, lm.perplexity(valid_data)))\n",
        "    print(\"{}-gram LM perplexity on test  set: {}\".format(n, lm.perplexity(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgM5mePhIHiO"
      },
      "source": [
        "## Interpolation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:41.937643Z",
          "start_time": "2020-01-29T18:30:41.932047Z"
        },
        "id": "5f57uTZvIHiO"
      },
      "outputs": [],
      "source": [
        "class InterpolateNGramLM(NGramLM): #Hacer slide interpolation\n",
        "\n",
        "    def __init__(self, N):\n",
        "        super(InterpolateNGramLM, self).__init__(N)\n",
        "        self.ngram_lms = []\n",
        "        self.lambdas = []\n",
        "\n",
        "    def train(self, vocab, data, smoothing_k=0, lambdas=[]):\n",
        "        assert len(lambdas) == self.N\n",
        "        assert sum(lambdas) - 1 < 1e-9\n",
        "        self.vocab = vocab\n",
        "        self.lambdas = lambdas\n",
        "\n",
        "        for i in range(self.N, 0, -1):\n",
        "            lm = NGramLM(i)\n",
        "            print(\"Training {}-gram language model\".format(i))\n",
        "            lm.train(vocab, data, smoothing_k)\n",
        "            self.ngram_lms.append(lm)\n",
        "\n",
        "    def get_ngram_logprob(self, word, seq_len, context):\n",
        "        prob = 0\n",
        "        for i, (coef, lm) in enumerate(zip(self.lambdas, self.ngram_lms)):\n",
        "            context_words = context.split()\n",
        "            cutted_context = \" \".join(context_words[-self.N + i + 1:])\n",
        "            prob += coef * lm.get_ngram_prob(context=cutted_context, word=word)\n",
        "        return math.log(prob) / seq_len\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:46.507705Z",
          "start_time": "2020-01-29T18:30:41.939076Z"
        },
        "id": "6TafZ7KpIHiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3027f8-5840-4f32-f46f-2d07620f3b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 3-gram language model\n",
            "Training 2-gram language model\n",
            "Training 1-gram language model\n"
          ]
        }
      ],
      "source": [
        "ilm = InterpolateNGramLM(3) #Aprende weights\n",
        "ilm.train(vocab, train_data, lambdas=[0.5, 0.4, 0.1]) #Valores arbitrarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:48.376911Z",
          "start_time": "2020-01-29T18:30:46.509150Z"
        },
        "id": "UZGf_gQOIHiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e252ee-c1df-485d-8b2b-a18fb588f30d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116.44308000312209\n",
            "95.62538365005771\n"
          ]
        }
      ],
      "source": [
        "print(ilm.perplexity(valid_data))\n",
        "print(ilm.perplexity(test_data))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit ('py38_pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "version": 3,
    "vscode": {
      "interpreter": {
        "hash": "ff27035c8dc0a26468b79942def09685664b2e815f4bca7e9395dcaceb48c986"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}